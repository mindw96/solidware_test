{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('solid': conda)"
  },
  "interpreter": {
   "hash": "afcf0587a0b9d8ad376f2b267ff70998ceccd3030669e3e753cc259109bd3fa3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "source": [
    "# 1. Data Load"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('data/winequality-red.csv', mode='rt')\n",
    "dataset = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['\"fixed acidity\"', '\"volatile acidity\"', '\"citric acid\"', '\"residual sugar\"', '\"chlorides\"', '\"free sulfur dioxide\"', '\"total sulfur dioxide\"', '\"density\"', '\"pH\"', '\"sulphates\"', '\"alcohol\"', '\"quality\"']\n"
     ]
    }
   ],
   "source": [
    "print(data.readline().strip().split(';'))\n",
    "for i in range(1599):\n",
    "    temp = data.readline().strip().split(';')\n",
    "    dataset.append(temp[:11])\n",
    "    labels.append(temp[-1])\n",
    "print(np.shape(dataset))\n",
    "print(np.shape(labels))\n",
    "data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.array(dataset, dtype=np.float32)\n",
    "labels = np.array(labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1599, 11)\n(1599,)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = dataset[:1232].copy()\n",
    "train_y = labels[:1232].copy()\n",
    "test_x = dataset[1232:].copy()\n",
    "test_y = labels[1232:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1232, 11)\n(1232,)\n(367, 11)\n(367,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "source": [
    "## 1.1 Standard Normalize "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 7.4     0.7     0.      1.9     0.076  11.     34.      0.9978  3.51\n  0.56    9.4   ]\n[-0.68604493  1.0223137  -1.5007055  -0.5221455  -0.27282238 -0.42211187\n -0.37999126  0.40999055  1.3628328  -0.6086837  -0.9083458 ]\n"
     ]
    }
   ],
   "source": [
    "print(train_x[0])\n",
    "\n",
    "train_x_std = (train_x - np.mean(train_x, axis=0)) / np.std(train_x, axis=0)\n",
    "\n",
    "print(train_x_std[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 7.6      0.43     0.29     2.1      0.075   19.      66.       0.99718\n  3.4      0.64     9.5    ]\n[-0.57341063 -0.4893766  -0.02046837 -0.36273965 -0.29374978  0.3597589\n  0.56779927  0.07982707  0.6549689  -0.15118532 -0.81657326]\n"
     ]
    }
   ],
   "source": [
    "print(test_x[0])\n",
    "\n",
    "test_x_std = (test_x - np.mean(train_x, axis=0)) / np.std(train_x, axis=0)\n",
    "\n",
    "print(test_x_std[0])"
   ]
  },
  {
   "source": [
    "## 1.2 MinMax Scaling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 7.4     0.7     0.      1.9     0.076  11.     34.      0.9978  3.51\n  0.56    9.4   ]\n[0.24778764 0.47933882 0.         0.06849315 0.10684474 0.14925373\n 0.09893993 0.58872557 0.663793   0.13772455 0.15384616]\n"
     ]
    }
   ],
   "source": [
    "print(train_x[0])\n",
    "\n",
    "train_x_mm = (train_x - np.min(train_x, axis=0)) / (np.max(train_x, axis=0) - np.min(train_x, axis=0))\n",
    "\n",
    "print(train_x_mm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 7.6      0.43     0.29     2.1      0.075   19.      66.       0.99718\n  3.4      0.64     9.5    ]\n[0.26548675 0.25619835 0.29       0.08219177 0.1051753  0.26865673\n 0.21201414 0.54150516 0.56896555 0.18562873 0.16923083]\n"
     ]
    }
   ],
   "source": [
    "print(test_x[0])\n",
    "\n",
    "test_x_mm = (test_x - np.min(train_x, axis=0)) / (np.max(train_x, axis=0) - np.min(train_x, axis=0))\n",
    "\n",
    "print(test_x_mm[0])"
   ]
  },
  {
   "source": [
    "# 2. Model Init"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression():\n",
    "    def __init__(self):\n",
    "        print('Logistic Regression Model Create')\n",
    "    # weight와 bias를 초기화해주는 함수\n",
    "    def init_model(self):\n",
    "        global weight, bias\n",
    "\n",
    "        weight = np.random.normal(0, 0.005, [11, 1])\n",
    "        bias = np.zeros([1])\n",
    "    # 학습을 하는 함수\n",
    "    def train(self, epoch_count, mb_size, report):\n",
    "        step_count = self.arrange_data(mb_size)\n",
    "        for epoch in range(epoch_count):\n",
    "            losses, mses = [], []\n",
    "            for n in range(step_count):\n",
    "                train_x, train_y = self.get_train_data(mb_size, n)\n",
    "                loss, mse = self.run_train(train_x, train_y)\n",
    "                losses.append(loss)\n",
    "                mses.append(mse)\n",
    "            if report > 0 and (epoch + 1) % report == 0:\n",
    "                print('Epoch {}: loss={:5.3f}, mse={:5.3f}'.format(epoch + 1, np.mean(losses), np.mean(mses)))\n",
    "\n",
    "    # 배치 사이즈를 통해 미니 배치 개수를 정하는 함수\n",
    "    def arrange_data(self, mb_size):\n",
    "        global data, lablel_data, shuffle_map\n",
    "\n",
    "        shuffle_map = np.arange(data.shape[0])\n",
    "        np.random.shuffle(shuffle_map)\n",
    "\n",
    "        step_count = int(data.shape[0] ) // mb_size\n",
    "\n",
    "        return step_count\n",
    "\n",
    "    # Train 데이터셋에서 shuffle map을 인덱스로 활용하여 epoch마다 랜덤하게 학습할 수 있도록 미니 배치 데이터셋을 생성한다.\n",
    "    def get_train_data(self, mb_size, nth):\n",
    "        global data, label_data, shuffle_map\n",
    "\n",
    "        if nth == 0:\n",
    "            np.random.shuffle(shuffle_map[:])\n",
    "\n",
    "        label_data = np.reshape(label_data, (-1,1))\n",
    "        train_data = data[shuffle_map[mb_size * nth:mb_size * (nth + 1)]]\n",
    "        train_label = label_data[shuffle_map[mb_size * nth:mb_size * (nth + 1)]]\n",
    "\n",
    "        return train_data, train_label\n",
    "\n",
    "    # 테스트 데이터를 설정해주는 함수\n",
    "    def get_test_data():\n",
    "        global data, shuffle_map, test_begin_idx, output_cnt\n",
    "\n",
    "        test_data = data[shuffle_map[test_begin_idx:]]\n",
    "\n",
    "        return test_data[:, :-output_cnt], test_data[:, -output_cnt:]\n",
    "\n",
    "    # 순전파부터 손실함수, 정확도를 구하고 역전파를 통해 weight와 bias를 업데이트하는 함수\n",
    "    def run_train(self, x, y):\n",
    "        output, aux_nn = self.forward_neuralnet(x)\n",
    "        loss, aux_pp = self.forward_postproc(output, y)\n",
    "        mse = self.evaluate(output, y)\n",
    "\n",
    "        G_loss = 1.0\n",
    "        G_output = self.backprop_postproc(G_loss, aux_pp)\n",
    "        self.backprop_neuralnet(G_output, aux_nn)\n",
    "\n",
    "        return loss, mse\n",
    "\n",
    "    # 테스트를 실행해주는 함수\n",
    "    def run_test(self, x, y):\n",
    "        output, _ = forward_neuralnet(x)\n",
    "        accuracy = eval_accuracy(output, y)\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    # 순전파를 처리하는 함수로 y=wx + b\n",
    "    def forward_neuralnet(self, x):\n",
    "        global weight, bias\n",
    "        \n",
    "        output = np.matmul(x, weight) + bias\n",
    "\n",
    "        return output, x\n",
    "\n",
    "    # loss를 계산하는 함수\n",
    "    def forward_postproc(self, output, y):\n",
    "        diff = output - y\n",
    "        square = np.square(diff)\n",
    "        loss = np.mean(square)\n",
    "\n",
    "        return loss, diff\n",
    "\n",
    "    # weight와 bias를 업데이트\n",
    "    def backprop_neuralnet(self, G_output, x):\n",
    "        global weight, bias\n",
    "        \n",
    "        g_output_w = x.transpose()\n",
    "        G_w = np.matmul(g_output_w, G_output)\n",
    "        G_b = np.sum(G_output, axis=0)\n",
    "        weight -= learningrate * G_w\n",
    "        bias -= learningrate * G_b\n",
    "\n",
    "    # Output의 손실기울기값을 구하는 함수\n",
    "    def backprop_postproc(self, G_loss, diff):\n",
    "        shape = diff.shape\n",
    "\n",
    "        g_loss_square = np.ones(shape) / np.prod(shape)\n",
    "        g_square_diff = 2 * diff\n",
    "        g_diff_output = 1\n",
    "\n",
    "        G_square = g_loss_square * G_loss\n",
    "        G_diff = g_square_diff * G_square\n",
    "        G_output = g_diff_output * G_diff\n",
    "\n",
    "        return G_output\n",
    "    \n",
    "    # MSE를 활용한 평가 \n",
    "    def evaluate(self, output, y):\n",
    "        mse = np.sum((y - output)**2) / len(output)\n",
    "\n",
    "        return mse\n",
    "\n",
    "    # 학습을 시작하는 함수\n",
    "    def start(self, epoch_count=50, mb_size=150, report=1, dataset=[], label=[]):\n",
    "        global data, label_data\n",
    "\n",
    "        data = dataset\n",
    "        label_data = label\n",
    "\n",
    "        self.init_model()\n",
    "        self.train(epoch_count, mb_size, report)\n",
    "\n",
    "    def return_param(self):\n",
    "        return weight, bias"
   ]
  },
  {
   "source": [
    "# 3. Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "learningrate = 0.0005\n",
    "epoch = 5000\n",
    "batch_size = 1000\n",
    "report = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Logistic Regression Model Create\n",
      "Epoch 100: loss=26.966, mse=26.966\n",
      "Epoch 200: loss=22.056, mse=22.056\n",
      "Epoch 300: loss=18.200, mse=18.200\n",
      "Epoch 400: loss=15.039, mse=15.039\n",
      "Epoch 500: loss=12.392, mse=12.392\n",
      "Epoch 600: loss=10.145, mse=10.145\n",
      "Epoch 700: loss=8.327, mse=8.327\n",
      "Epoch 800: loss=6.991, mse=6.991\n",
      "Epoch 900: loss=5.747, mse=5.747\n",
      "Epoch 1000: loss=4.780, mse=4.780\n",
      "Epoch 1100: loss=4.071, mse=4.071\n",
      "Epoch 1200: loss=3.401, mse=3.401\n",
      "Epoch 1300: loss=2.802, mse=2.802\n",
      "Epoch 1400: loss=2.368, mse=2.368\n",
      "Epoch 1500: loss=2.081, mse=2.081\n",
      "Epoch 1600: loss=1.686, mse=1.686\n",
      "Epoch 1700: loss=1.451, mse=1.451\n",
      "Epoch 1800: loss=1.281, mse=1.281\n",
      "Epoch 1900: loss=1.128, mse=1.128\n",
      "Epoch 2000: loss=1.027, mse=1.027\n",
      "Epoch 2100: loss=0.871, mse=0.871\n",
      "Epoch 2200: loss=0.819, mse=0.819\n",
      "Epoch 2300: loss=0.722, mse=0.722\n",
      "Epoch 2400: loss=0.677, mse=0.677\n",
      "Epoch 2500: loss=0.620, mse=0.620\n",
      "Epoch 2600: loss=0.582, mse=0.582\n",
      "Epoch 2700: loss=0.560, mse=0.560\n",
      "Epoch 2800: loss=0.532, mse=0.532\n",
      "Epoch 2900: loss=0.517, mse=0.517\n",
      "Epoch 3000: loss=0.479, mse=0.479\n",
      "Epoch 3100: loss=0.477, mse=0.477\n",
      "Epoch 3200: loss=0.479, mse=0.479\n",
      "Epoch 3300: loss=0.446, mse=0.446\n",
      "Epoch 3400: loss=0.436, mse=0.436\n",
      "Epoch 3500: loss=0.459, mse=0.459\n",
      "Epoch 3600: loss=0.427, mse=0.427\n",
      "Epoch 3700: loss=0.413, mse=0.413\n",
      "Epoch 3800: loss=0.432, mse=0.432\n",
      "Epoch 3900: loss=0.430, mse=0.430\n",
      "Epoch 4000: loss=0.420, mse=0.420\n",
      "Epoch 4100: loss=0.389, mse=0.389\n",
      "Epoch 4200: loss=0.410, mse=0.410\n",
      "Epoch 4300: loss=0.406, mse=0.406\n",
      "Epoch 4400: loss=0.407, mse=0.407\n",
      "Epoch 4500: loss=0.399, mse=0.399\n",
      "Epoch 4600: loss=0.409, mse=0.409\n",
      "Epoch 4700: loss=0.414, mse=0.414\n",
      "Epoch 4800: loss=0.411, mse=0.411\n",
      "Epoch 4900: loss=0.396, mse=0.396\n",
      "Epoch 5000: loss=0.412, mse=0.412\n"
     ]
    }
   ],
   "source": [
    "LR = Regression()\n",
    "LR.start(epoch, batch_size, report, train_x_std, train_y)"
   ]
  },
  {
   "source": [
    "# 4. Testset Evaluate"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 0.06474432]\n [-0.17244627]\n [-0.00340037]\n [ 0.04602996]\n [-0.08642884]\n [ 0.02651111]\n [-0.12867671]\n [-0.09104238]\n [-0.02458762]\n [ 0.13889926]\n [ 0.27652147]] [5.63630294]\n"
     ]
    }
   ],
   "source": [
    "trained_weight, trained_bias = LR.return_param()\n",
    "print(trained_weight, trained_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for idx in range(len(test_x_std)):\n",
    "    predict = np.matmul(test_x_std[idx], trained_weight) + trained_bias\n",
    "    predictions.append(predict[0])\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.4619600607212303 0.6796764382566386 0.5100464586475405 51.00464586475405\n"
     ]
    }
   ],
   "source": [
    "mse = np.sum((test_y - predictions)**2) / len(predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = np.sum(np.abs(test_y - predictions)) / len(predictions)\n",
    "mape = mae * 100\n",
    "print(mse, rmse, mae, mape)"
   ]
  },
  {
   "source": [
    "MinMax"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningrate = 0.001\n",
    "epoch = 50000\n",
    "batch_size = 1000\n",
    "report = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Logistic Regression Model Create\n",
      "Epoch 1000: loss=0.776, mse=0.776\n",
      "Epoch 2000: loss=0.734, mse=0.734\n",
      "Epoch 3000: loss=0.667, mse=0.667\n",
      "Epoch 4000: loss=0.662, mse=0.662\n",
      "Epoch 5000: loss=0.656, mse=0.656\n",
      "Epoch 6000: loss=0.592, mse=0.592\n",
      "Epoch 7000: loss=0.604, mse=0.604\n",
      "Epoch 8000: loss=0.570, mse=0.570\n",
      "Epoch 9000: loss=0.581, mse=0.581\n",
      "Epoch 10000: loss=0.541, mse=0.541\n",
      "Epoch 11000: loss=0.540, mse=0.540\n",
      "Epoch 12000: loss=0.553, mse=0.553\n",
      "Epoch 13000: loss=0.511, mse=0.511\n",
      "Epoch 14000: loss=0.515, mse=0.515\n",
      "Epoch 15000: loss=0.519, mse=0.519\n",
      "Epoch 16000: loss=0.493, mse=0.493\n",
      "Epoch 17000: loss=0.524, mse=0.524\n",
      "Epoch 18000: loss=0.513, mse=0.513\n",
      "Epoch 19000: loss=0.484, mse=0.484\n",
      "Epoch 20000: loss=0.474, mse=0.474\n",
      "Epoch 21000: loss=0.480, mse=0.480\n",
      "Epoch 22000: loss=0.496, mse=0.496\n",
      "Epoch 23000: loss=0.491, mse=0.491\n",
      "Epoch 24000: loss=0.474, mse=0.474\n",
      "Epoch 25000: loss=0.484, mse=0.484\n",
      "Epoch 26000: loss=0.464, mse=0.464\n",
      "Epoch 27000: loss=0.483, mse=0.483\n",
      "Epoch 28000: loss=0.464, mse=0.464\n",
      "Epoch 29000: loss=0.454, mse=0.454\n",
      "Epoch 30000: loss=0.493, mse=0.493\n",
      "Epoch 31000: loss=0.463, mse=0.463\n",
      "Epoch 32000: loss=0.444, mse=0.444\n",
      "Epoch 33000: loss=0.469, mse=0.469\n",
      "Epoch 34000: loss=0.445, mse=0.445\n",
      "Epoch 35000: loss=0.466, mse=0.466\n",
      "Epoch 36000: loss=0.467, mse=0.467\n",
      "Epoch 37000: loss=0.452, mse=0.452\n",
      "Epoch 38000: loss=0.466, mse=0.466\n",
      "Epoch 39000: loss=0.435, mse=0.435\n",
      "Epoch 40000: loss=0.456, mse=0.456\n",
      "Epoch 41000: loss=0.441, mse=0.441\n",
      "Epoch 42000: loss=0.436, mse=0.436\n",
      "Epoch 43000: loss=0.448, mse=0.448\n",
      "Epoch 44000: loss=0.445, mse=0.445\n",
      "Epoch 45000: loss=0.435, mse=0.435\n",
      "Epoch 46000: loss=0.446, mse=0.446\n",
      "Epoch 47000: loss=0.441, mse=0.441\n",
      "Epoch 48000: loss=0.463, mse=0.463\n",
      "Epoch 49000: loss=0.455, mse=0.455\n",
      "Epoch 50000: loss=0.450, mse=0.450\n"
     ]
    }
   ],
   "source": [
    "LR = Regression()\n",
    "LR.start(epoch, batch_size, report, train_x_mm, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 0.76197825]\n [-0.66820886]\n [ 0.39257592]\n [ 0.05230968]\n [-0.07431078]\n [ 0.09000612]\n [-0.53036151]\n [ 0.00585235]\n [ 1.03607366]\n [ 1.02441618]\n [ 2.16745029]] [4.18753271]\n"
     ]
    }
   ],
   "source": [
    "trained_weight, trained_bias = LR.return_param()\n",
    "print(trained_weight, trained_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for idx in range(len(test_x_mm)):\n",
    "    predict = np.matmul(test_x_mm[idx], trained_weight) + trained_bias\n",
    "    predictions.append(predict[0])\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.4964977568487878 0.7046259694680489 0.5237623574220684 52.376235742206845\n"
     ]
    }
   ],
   "source": [
    "mse = np.sum((test_y - predictions)**2) / len(predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = np.sum(np.abs(test_y - predictions)) / len(predictions)\n",
    "mape = mae * 100\n",
    "print(mse, rmse, mae, mape)"
   ]
  },
  {
   "source": [
    "Result  \n",
    "\n",
    "Best Custom Regression Model MSE : 0.4620  \n",
    "Best Preprocessing : Standard Scaling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}